## NOTE: This code is for checking the scores assigned to the experts.

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from tqdm import tqdm
from datasets import load_dataset
###-------- Basic settings --------####
torch.set_default_device("cuda:0") # or "cpu"
torch.set_grad_enabled(False)
cur_model = 'olmoe' # 'olmoe', 'mixtral', 'qwen'
## NOTE: output_router_logits is not supported for 'deepseek' in transformers==4.52.4.
####-------- Model settings --------####
if cur_model == 'olmoe':
    model_id = "allenai/OLMoE-1B-7B-0924" # "allenai/OLMoE-1B-7B-0125"
    n_layers = 16
    n_dim = 2048
    n_experts = 64
    top_k = 8
elif cur_model == 'deepseek':
    model_id = "deepseek-ai/DeepSeek-V2-Lite"
    n_layers = 27
    n_dim = 2048
    n_experts = 64
    top_k = 6
elif cur_model == 'mixtral':
    model_id = "mistralai/Mixtral-8x7B-v0.1"
    n_layers = 32
    n_dim = 4096
    n_experts = 8
    top_k = 2
elif cur_model == 'qwen':
    model_id = "Qwen/Qwen3-30B-A3B"
    n_layers = 48
    n_dim = 2048
    n_experts = 128
    top_k = 8
####-------- Model loading --------####
model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation="eager")
tokenizer = AutoTokenizer.from_pretrained(model_id)
####-------- Functions --------####
def c4_dataset_helper(dataset_len, min_words):
    """ Select some data from C4 dataset as test samples. """
    original_dataset = load_dataset(path='allenai/c4', data_files="en/c4-train.00001-of-01024.json.gz") ## len: 356318
    my_dataset = []
    counter = 0 ## TODO: make it more random
    while len(my_dataset) < dataset_len:
        cur_text = original_dataset["train"][counter]["text"]
        if len(cur_text.split()) >= min_words: ## to ensure that the prompt is not too short
            my_dataset.append(cur_text)
        counter += 1
    print(len(my_dataset))
    del original_dataset
    return my_dataset

def run(prompt_ls, model, tokenizer, bsz=100, max_length=32):
    """ Run the model and get the scores.
    :param1 prompt_ls: list of prompts
    :param4 bsz: batch size
    :param5 max_length: the ACTUAL length of each prompt. should ensure that the prompts are longer enough to be truncated.
    """
    batch_token = tokenizer(prompt_ls, return_tensors="pt", max_length=max_length, padding=False, truncation=True)
    n_prompts = len(prompt_ls)
    print(tokenizer.batch_decode(batch_token["input_ids"][0])) # example: print the first prompt
    for B in tqdm(range(0, n_prompts, bsz)):
        model_outputs = model(input_ids=batch_token['input_ids'][B:B+bsz], attention_mask=batch_token['attention_mask'][B:B+bsz], output_router_logits=True)
        expert_score = model_outputs[3] # tuple, len = n_layers
        expert_score = torch.stack(expert_score, dim=0)
        n_layers, _, n_experts = expert_score.shape
        expert_score = expert_score.reshape(n_layers, -1, max_length, n_experts) # shape: [n_layers, n_prompts, n_tokens, n_experts]

        # for test
        top_n_experts = torch.argsort(expert_score, dim=3, descending=True)[:,:,:, :8] # get top-8 experts
        print("expert_score.shape", expert_score.shape)
        print("expert_score[0, 2, 1, :]", expert_score[0, 2, 1, :])
        print("top_n_experts[0, 2, 1, :]", top_n_experts[0, 2, 1, :])
    
####-------- Test code --------####
# my_c4_dataset = c4_dataset_helper(dataset_len=10, min_words=50) # dataset_len=5000
# run(my_c4_dataset, model, tokenizer, bsz=100, max_length=32)
prompt_longsentence = "In machine learning, diffusion models, also known as diffusion-based generative models or score-based generative models, are a class of latent variable generative models. A diffusion model consists of two major components: the forward diffusion process, and the reverse sampling process. The goal of diffusion models is to learn a diffusion process for a given dataset, such that the process can generate new elements that are distributed similarly as the original dataset. A diffusion model models data as generated by a diffusion process, whereby a new datum performs a random walk with drift through the space of all possible data."
run([prompt_longsentence]*100, model, tokenizer, bsz=100, max_length=32)

# The output should match the following result:
#  ['In', ' machine', ' learning', ',', ' diffusion', ' models', ',', ' also', ' known', ' as', ' diffusion', '-', 'based', ' gener', 'ative', ' models', ' or', ' score', '-', 'based', ' gener', 'ative', ' models', ',', ' are', ' a', ' class', ' of', ' latent', ' variable', ' gener', 'ative']
#  expert_score.shape torch.Size([16, 100, 32, 64])
#  expert_score[0, 2, 1, :] tensor([-0.2563, -0.6067, -0.2866, -0.5339, -0.5868, -1.0188,  0.6949, -1.4125,
#          -0.1977, -0.0346, -0.7053, -1.0664, -2.8447, -0.4771,  1.7898, -0.0778,
#          -0.2615,  0.5611, -1.1094, -2.1070, -0.4468, -0.2739, -2.4814, -0.6204,
#          -1.0243, -0.1866,  2.9565,  0.0549, -2.2698, -0.9345, -0.9147, -0.4220,
#          -0.5807, -0.0620, -2.0663, -0.1985, -0.7091, -0.2198, -0.4348, -0.3085,
#          -0.9570, -1.0319, -1.0329, -1.2994, -0.6700,  0.0754, -1.1656, -0.1296,
#          -0.7994, -0.2970, -0.5266, -0.0370,  0.2275, -1.3549, -0.4351, -0.6719,
#          -0.3611, -0.5907, -1.9266, -0.2232, -0.9368, -0.3309, -0.2454, -0.4561],
#         device='cuda:0')
#  top_n_experts[0, 2, 1, :] tensor([26, 14,  6, 17, 52, 45, 27,  9], device='cuda:0')
