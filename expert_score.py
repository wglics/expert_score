## NOTE: This code is for checking the scores assigned to the experts.

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from tqdm import tqdm
from datasets import load_dataset
###-------- Basic settings --------####
torch.set_default_device("cuda:0") # or "cpu"
torch.set_grad_enabled(False)
cur_model = 'olmoe' # 'olmoe', 'mixtral', 'qwen'
## NOTE: output_router_logits is not supported for 'deepseek' in transformers==4.52.4.
####-------- Model settings --------####
if cur_model == 'olmoe':
    model_id = "allenai/OLMoE-1B-7B-0924" # "allenai/OLMoE-1B-7B-0125"
    n_layers = 16
    n_dim = 2048
    n_experts = 64
    top_k = 8
elif cur_model == 'deepseek':
    model_id = "deepseek-ai/DeepSeek-V2-Lite"
    n_layers = 27
    n_dim = 2048
    n_experts = 64
    top_k = 6
elif cur_model == 'mixtral':
    model_id = "mistralai/Mixtral-8x7B-v0.1"
    n_layers = 32
    n_dim = 4096
    n_experts = 8
    top_k = 2
elif cur_model == 'qwen':
    model_id = "Qwen/Qwen3-30B-A3B"
    n_layers = 48
    n_dim = 2048
    n_experts = 128
    top_k = 8
####-------- Model loading --------####
model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation="eager")
tokenizer = AutoTokenizer.from_pretrained(model_id)
####-------- Functions --------####
def c4_dataset_helper(dataset_len, min_words):
    """ Select some data from C4 dataset as test samples. """
    original_dataset = load_dataset(path='allenai/c4', data_files="en/c4-train.00001-of-01024.json.gz") ## len: 356318
    my_dataset = []
    counter = 0 ## TODO: make it more random
    while len(my_dataset) < dataset_len:
        cur_text = original_dataset["train"][counter]["text"]
        if len(cur_text.split()) >= min_words: ## to ensure that the prompt is not too short
            my_dataset.append(cur_text)
        counter += 1
    print(len(my_dataset))
    del original_dataset
    return my_dataset

def run(prompt_ls, model, tokenizer, bsz=100, max_length=32):
    """ Run the model and get the scores.
    :param1 prompt_ls: list of prompts
    :param4 bsz: batch size
    :param5 max_length: the ACTUAL length of each prompt. should ensure that the prompts are longer enough to be truncated.
    """
    batch_token = tokenizer(prompt_ls, return_tensors="pt", max_length=max_length, padding=False, truncation=True)
    n_prompts = len(prompt_ls)
    print(tokenizer.batch_decode(batch_token["input_ids"][0])) # example: print the first prompt
    for B in tqdm(range(0, n_prompts, bsz)):
        model_outputs = model(input_ids=batch_token['input_ids'][B:B+bsz], attention_mask=batch_token['attention_mask'][B:B+bsz], output_router_logits=True)
        expert_score = model_outputs[3] # tuple, len = n_layers
        expert_score = torch.stack(expert_score, dim=0)
        n_layers, _, n_experts = expert_score.shape
        expert_score = expert_score.reshape(n_layers, -1, max_length, n_experts) # shape: [n_layers, n_prompts, n_tokens, n_experts]

        # for test
        top_n_experts = torch.argsort(expert_score, dim=3, descending=True)[:,:,:, :8] # get top-8 experts
        print("expert_score.shape", expert_score.shape)
        print("expert_score[0, 2, 1, :]", expert_score[0, 2, 1, :])
        print("top_n_experts[0, 2, 1, :]", top_n_experts[0, 2, 1, :])
    
####-------- Test code --------####
# my_c4_dataset = c4_dataset_helper(dataset_len=10, min_words=50) # dataset_len=5000
# run(my_c4_dataset, model, tokenizer, bsz=100, max_length=32)
prompt_longsentence = "In machine learning, diffusion models, also known as diffusion-based generative models or score-based generative models, are a class of latent variable generative models. A diffusion model consists of two major components: the forward diffusion process, and the reverse sampling process. The goal of diffusion models is to learn a diffusion process for a given dataset, such that the process can generate new elements that are distributed similarly as the original dataset. A diffusion model models data as generated by a diffusion process, whereby a new datum performs a random walk with drift through the space of all possible data."
run([prompt_longsentence]*100, model, tokenizer, bsz=100, max_length=32)

